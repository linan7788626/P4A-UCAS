{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9bc6c2-3882-4c8c-84d4-514483f1b68b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parameter estimation with Markov chain Monte Carlo\n",
    "http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2015/tutorials/t4b_param_est_with_mcmc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba07ce1-4da4-4264-ad45-4582ccb2f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Our numerical workhorses\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import scipy.stats as st\n",
    "\n",
    "# Numerical differentiation packages\n",
    "import numdifftools as ndt\n",
    "\n",
    "# Our main MCMC package\n",
    "import emcee\n",
    "\n",
    "# Import pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn, useful for graphics\n",
    "import seaborn as sns\n",
    "\n",
    "# Corner is useful for displaying MCMC results\n",
    "import corner\n",
    "\n",
    "# Magic function to make matplotlib inline; other style specs must come AFTER\n",
    "%matplotlib inline\n",
    "\n",
    "# This enables high res graphics inline (only use with static plots (non-Bokeh))\n",
    "# SVG is preferred, but there is a bug in Jupyter with vertical lines\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce834a2f-b7d9-4fd9-ad40-d2b8ecdbfc81",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to use Markov chain Monte Carlo to do parameter estimation. To get the basic idea behind MCMC, imagine for a moment that we can draw samples out of the posterior distribution. This means that the probability of choosing given values of a set of parameters is proportional to the posterior probability of that set of values. If we drew many many such samples, we could reconstruct the posterior from the samples, e.g., by making histograms. That's a big thing to image: that we can draw properly weighted samples. But, it turns out that we can! That is what MCMC allows us to do.\n",
    "\n",
    "We will discuss the theory behind this seemingly miraculous capability in lecture. For today, we will just use the fact that we can do the sampling to learn about posterior distributions in the context of parameter estimation. We will use the emcee package to do our sampling by MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e06257-718e-4b8e-bc3c-5311721e4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = pd.read_csv('../data/singer_et_al/singer_transcript_counts.csv',\n",
    "                comment='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1ed41-0c1d-440c-bbf5-fd2c86f09d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "sp_inds = list(itertools.product([0,1], [0,1]))\n",
    "\n",
    "for i, gene in enumerate(df.columns):\n",
    "    # Build ECDF\n",
    "    y = np.arange(len(df[gene])) / len(df[gene])\n",
    "    x = np.sort(df[gene].values)\n",
    "    \n",
    "    # Plot\n",
    "    ax[sp_inds[i]].plot(x, y, '.')\n",
    "    ax[sp_inds[i]].text(0.7, 0.25, gene, transform=ax[sp_inds[i]].transAxes,\n",
    "                       fontsize=18)\n",
    "    ax[sp_inds[i]].margins(0.02)\n",
    "    \n",
    "# Clean up\n",
    "for i in [0,1]:\n",
    "    ax[1,i].set_xlabel('mRNA count')\n",
    "    ax[i,0].set_ylabel('ECDF')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae3a9a-77fc-45d9-b432-924df1b51666",
   "metadata": {},
   "source": [
    "The posterior\n",
    "We derived last time that the posterior distributions for a single negative binomial and double-binomial model are respectively\n",
    "\n",
    "P(r,p∣n)P(r1,r2,p1,p2,f∣n)∝∏n∈n(n+r−1)!n!(r−1)!pr(1−p)n,∝f(n+r1−1)!n!(r1−1)!pr11(1−p1)n+(1−f)(n+r2−1)!n!(r2−1)!pr22(1−p2)n.\n",
    " \n",
    "As with our quest to find the MAP by optimization, we need to code up the log posterior for MCMC. Conveniently, emcee uses the same API for the posterior definition as the scipy.optimize does. We will again specify that  p1>p2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5ee65-32b6-4194-8e12-0ba8a0e7d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(params, n):\n",
    "    \"\"\"\n",
    "    Log posterior for MLE of Singer data.\n",
    "    \"\"\"\n",
    "    r, p = params\n",
    "    \n",
    "    # Zero probability of having p < 0 or p > 1\n",
    "    if p < 0 or p > 1:\n",
    "        return -np.inf\n",
    "    \n",
    "    # Zero probability of r < 0\n",
    "    if r < 0:\n",
    "        return -np.inf\n",
    "\n",
    "    return st.nbinom.logpmf(n, r, p).sum()\n",
    "\n",
    "    \n",
    "def neg_log_posterior(params, n):\n",
    "    \"\"\"\n",
    "    Negative log posterior for MLE of Singer data.\n",
    "    \"\"\"\n",
    "    return -log_posterior(params, n)\n",
    "\n",
    "\n",
    "def log_posterior_bimodal(params, n):\n",
    "    \"\"\"\n",
    "    Log of posterior for linear combination of neg. binomials.\n",
    "    \"\"\"\n",
    "    r_1, r_2, p_1, p_2, f = params\n",
    "    \n",
    "    if (f < 0) or (f > 1):\n",
    "        return -np.inf\n",
    "    \n",
    "    if (r_1 < 0) or (r_2 < 0) or (p_1 < p_2) or (p_2 < 0) or (p_1 > 1):\n",
    "        return -np.inf\n",
    "    \n",
    "    return np.log(f * st.nbinom.pmf(n, r_1, p_1)\n",
    "                  + (1-f) * st.nbinom.pmf(n, r_2, p_2)).sum()\n",
    "\n",
    "\n",
    "def neg_log_posterior_bimodal(params, n):\n",
    "    \"\"\"\n",
    "    Negative log posterior for linear combination of neg. binomials.\n",
    "    \"\"\"\n",
    "    return -log_posterior_bimodal(params, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b58a1-f7a9-4eb1-8b7b-286a71257dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2        # number of parameters in the model (r and p)\n",
    "n_walkers = 50   # number of MCMC walkers\n",
    "n_burn = 500     # \"burn-in\" period to let chains stabilize\n",
    "n_steps = 5000   # number of MCMC steps to take after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc102dd8-a931-4493-9713-e45b2603758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03665001-7522-4358-807c-19340a350749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p0[i,j] is the starting point for walk i along variable j.\n",
    "p0 = np.empty((n_walkers, n_dim))\n",
    "p0[:,0] = np.random.exponential(0.1, n_walkers)            # r\n",
    "p0[:,1] = np.random.uniform(0, 1, n_walkers)             # p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0f147-1aab-478f-a72f-ae0c09431eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, log_posterior, \n",
    "                                args=(df['Prdm14'],), threads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df64f5a-7b25-4ee9-865d-f32892eb51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do burn-in\n",
    "pos, prob, state = sampler.run_mcmc(p0, n_burn, storechain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ca20c-98b9-4799-9337-6a8f431ebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample again, starting from end burn-in state\n",
    "_ = sampler.run_mcmc(pos, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ca75e-52ca-47d6-b455-c31fc4a92b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.chain.shape)\n",
    "print(sampler.flatchain.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24235b6-1d2b-4fc4-8c60-3cffc22ebd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "for i in [0, 1]:\n",
    "    ax[i].plot(sampler.chain[0,:,i], 'k-', lw=0.2)\n",
    "    ax[i].plot([0, n_steps-1], \n",
    "             [sampler.chain[0,:,i].mean(), sampler.chain[0,:,i].mean()], 'r-')\n",
    "\n",
    "ax[1].set_xlabel('sample number')\n",
    "ax[0].set_ylabel('r')\n",
    "ax[1].set_ylabel('p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f82b6-7aee-4bf5-9b26-b324e056519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the most probable parameter set\n",
    "max_ind = np.argmax(sampler.flatlnprobability)\n",
    "\n",
    "# Pull out values.\n",
    "r_MAP, p_MAP = sampler.flatchain[max_ind,:]\n",
    "\n",
    "# Print the results\n",
    "print(\"\"\"\n",
    "Most probable parameter values:\n",
    "r:  {0:.3f}\n",
    "p: {1:.3f}\n",
    "\"\"\".format(r_MAP, p_MAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159659fd-4ac2-43c5-8b45-d384ef7e8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error bars by taking standard deviation\n",
    "r_err, p_err = sampler.flatchain.std(axis=0)\n",
    "\n",
    "print('Error bars:\\n', r_err, p_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafc985-23fe-41a2-a383-22a429a119e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "for i in [0, 1]:\n",
    "    # Plot the histogram as a step plot\n",
    "    _ = ax[i].hist(sampler.flatchain[:,i], bins=100, normed=True, \n",
    "                   histtype='step', lw=2)\n",
    "\n",
    "ax[0].set_xlabel('r')\n",
    "ax[1].set_xlabel('p')\n",
    "ax[0].set_ylabel('probability')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d3e5d-0d07-489f-8a9c-9caa61c21416",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(sampler.flatchain, labels=['r', 'p'], bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af4bf4-8bf6-457f-bb75-2ef8df42dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and how we start them\n",
    "params = collections.OrderedDict(\n",
    "        [('r1', (np.random.exponential, (1,))),\n",
    "         ('r2', (np.random.exponential, (1,))),\n",
    "         ('p1', (np.random.uniform, (0, 1))),\n",
    "         ('p2', (np.random.uniform, (0, 1))),\n",
    "         ('f', (np.random.uniform, (0, 1)))])\n",
    "\n",
    "# Define walker settings\n",
    "n_dim = len(params)\n",
    "n_walkers = 50\n",
    "n_burn = 500\n",
    "n_steps = 5000\n",
    "\n",
    "# Keep a set of param names handy\n",
    "param_names = list(params.keys())\n",
    "\n",
    "# Seed random number generator for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# p0[i,j] is the starting point for walk i along variable j.\n",
    "p0 = np.empty((n_walkers, n_dim))\n",
    "for i, key in enumerate(params):\n",
    "    p0[:,i] = params[key][0](*(params[key][1] + (n_walkers,)))\n",
    "\n",
    "# Make sure p1 > p2\n",
    "p0[:,2], p0[:,3] = np.maximum(p0[:,2], p0[:,3]), np.minimum(p0[:,2], p0[:,3])\n",
    "\n",
    "# Set up the EnsembleSampler instance\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, log_posterior_bimodal, \n",
    "                                args=(df['Rex1'],), threads=6)\n",
    "\n",
    "# Do burn-in\n",
    "pos, prob, state = sampler.run_mcmc(p0, n_burn, storechain=False)\n",
    "\n",
    "# Sample again, starting from end burn-in state\n",
    "_ = sampler.run_mcmc(pos, n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e8558-b671-4e84-859a-692a2fbab0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use triangle.corner to make summary plot\n",
    "fig = corner.corner(sampler.flatchain, labels=param_names, bins=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
